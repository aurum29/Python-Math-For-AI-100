{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13. 오차역전파.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFzXOpn4Vtf4puiLozeqr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zombe962/Python-Math-For-AI-100/blob/master/13.%20%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShdiF8Qq6Zcr"
      },
      "source": [
        "# Softmax와 Cross Entroopy Error\r\n",
        "\r\n",
        "```\r\n",
        "class SoftmaxWithLoss:\r\n",
        "    def __init__(self):\r\n",
        "        self.y = None   # 출력(계산 결과)\r\n",
        "        self.t = None   # 정답(MNIST레이블)\r\n",
        "    \r\n",
        "    def foward(self, x, t):\r\n",
        "        self.t = t\r\n",
        "        self.y = softmax(x)\r\n",
        "        result = cee(self.y, self.t)    # 엔트로피 오차 함수를 이용해 오차값\r\n",
        "        return result\r\n",
        "    \r\n",
        "    def backward(self, dout=1):     # 역전파 진행\r\n",
        "        batch_size = self.t.shape[0]\r\n",
        "        dx = (self.y - self.t) / batch_size\r\n",
        "        return dx\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bm2et6w7rx8"
      },
      "source": [
        "# 활성함수 Relu 클래스\r\n",
        "\r\n",
        "```\r\n",
        "class Relu:\r\n",
        "    def __init__(self):\r\n",
        "        self.mask = NOne\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        self.mask = (x <= 0)\r\n",
        "        result = x.copy()\r\n",
        "        result[self.mask] - 0\r\n",
        "        return result\r\n",
        "    \r\n",
        "    def backward(self, dout):\r\n",
        "        dlut[self.mask] = 0\r\n",
        "        dx = dout\r\n",
        "        return dx\r\n",
        "```\r\n",
        "\r\n",
        "```\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "x = np.array([[1,-2,-3],[2,-4,6]])\r\n",
        "print('x = \\n', x)\r\n",
        "\r\n",
        "mask = (x<=0)\r\n",
        "print('\\nmask = \\n', mask)\r\n",
        "\r\n",
        "x[mask] = 0\r\n",
        "print('\\nmodified x = \\n', x)\r\n",
        "```\r\n",
        "\r\n",
        "x = \r\n",
        " [[ 1 -2 -3]\r\n",
        " [ 2 -4  6]]\r\n",
        "\r\n",
        "mask = \r\n",
        " [[False  True  True]\r\n",
        " [False  True False]]\r\n",
        "\r\n",
        "modified x = \r\n",
        " [[1 0 0]\r\n",
        " [2 0 6]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW-hUCh_Akm2"
      },
      "source": [
        "# Affine 클래스\r\n",
        "```\r\n",
        "class Affine:\r\n",
        "    def __init__(self, W, b):\r\n",
        "        self.W = W  #W0, W1\r\n",
        "        self.b = b  #b0, b1\r\n",
        "        self.x = None\r\n",
        "        self.dW = None  #W0, W1의 기울기\r\n",
        "        self.db = None  #b0, b1의 기울기\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        self.x = x\r\n",
        "        result = np.dot(self.x, self.W) + self.b\r\n",
        "        return result\r\n",
        "    \r\n",
        "    def backward(self, dout):\r\n",
        "        dx = np.dot(dout, self.W.T)\r\n",
        "        self.dW = np.dot(self.x.T, dout)\r\n",
        "        self.db = np.sum(dout, axis=0)\r\n",
        "        return dx\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8egXMTLLE2ye"
      },
      "source": [
        "#오차역전파를 사용한 MNIST 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWcmpqFPExpV"
      },
      "source": [
        "#13.6.1. [Step1] 미분과 역전파 선택\r\n",
        "\r\n",
        "#오차역전파와 미분함수 중 선택\r\n",
        "#process = (미분사용 : 1, 역전파사용 : 2)\r\n",
        "\r\n",
        "process = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JfF_yOkFCUb",
        "outputId": "49b1f04e-711e-4011-ea71-4c07e048e19a"
      },
      "source": [
        "#13.6.2. [Step2] Mnist 데이터 가져오기\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\r\n",
        "t_trainlbl, t_testlbl = t_train, t_test\r\n",
        "\r\n",
        "#28x28을 784로 수정\r\n",
        "x_train = x_train.reshape(60000,784)    # 주석(1)\r\n",
        "x_test = x_test.reshape(10000,784)\r\n",
        "\r\n",
        "#one-hot label\r\n",
        "T0 = np.zeros((t_train.size, 10))   #(60000, 10) = 000\r\n",
        "T1 = np.zeros((t_test.size, 10))    #(10000, 10) = 000\r\n",
        "\r\n",
        "for idx in range(t_train.size): T0[idx][t_train[idx]] = 1   # (3)\r\n",
        "for idx in range(t_test.size): T1[idx][t_test[idx]] = 1\r\n",
        "\r\n",
        "t_train, t_test = T0, T1\r\n",
        "\r\n",
        "#normalize 0.0 ~ 1.0\r\n",
        "x_train = x_train / 225\r\n",
        "x_test = x_test / 255\r\n",
        "\r\n",
        "print('MNIST DataSets 준비 완료')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST DataSets 준비 완료\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33zGEtIIGVGk"
      },
      "source": [
        "#13.6.3. [STEP3] 함수 정의 : 수치미분, 소프트맥스, CEE\r\n",
        "\r\n",
        "#미분함수\r\n",
        "def numerical_diff(f,x):\r\n",
        "    h = 1e-4    #0.0001\r\n",
        "    nd_coef = np.zeros_like(x)\r\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags = ['readwrite'])\r\n",
        "    while not it.finished:\r\n",
        "        index = it.multi_indextmp = float(x[index])\r\n",
        "        x[index] = tmp+h\r\n",
        "        fxh2 = f()  #f(x+h)\r\n",
        "        x[index] = tmp-h\r\n",
        "        fxh1 = f()  #f(x-h)\r\n",
        "        nd_coef[index] = (fxh2 - fxh1) / (2*h)\r\n",
        "        x[index] = tmp\r\n",
        "        it.iternext()\r\n",
        "    return nd_coef\r\n",
        "\r\n",
        "#소프트맥스\r\n",
        "def softmax(x):\r\n",
        "    if x.ndim == 1: #기본 1개 처리과정, 백터 입력\r\n",
        "        x = x - np.max(x)\r\n",
        "        return np.exp(x) / np.sum(np.exp(x))\r\n",
        "    if x.ndim == 2: #배치용 n개 처리, 행렬입력\r\n",
        "        x = x.T - np.max(x.T, axis=0)\r\n",
        "        return (np.exp(x) / np.sum(np.exp(x), axis=0)).T\r\n",
        "\r\n",
        "#크로스엔트로피오차\r\n",
        "def cee(y, t):\r\n",
        "    if y.ndim == 1:\r\n",
        "        t = t.reshape(1, t.size)    #크기가 1xN인 2차원 행렬로 재구성\r\n",
        "        y = y.reshape(1, y.size)\r\n",
        "    result = -np.sum(t*np.log(y + 1e-7)) / y.shape[0]\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4qnkp4QI6J-"
      },
      "source": [
        "#13.6.4. [STEP4] 클래스 정의 : ReLU, Affine, SoftmaxWithLoss\r\n",
        "\r\n",
        "class Relu:\r\n",
        "    def __init__(self):\r\n",
        "        self.mask = None\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        self.mask = (x<=0)\r\n",
        "        result = x.copy()\r\n",
        "        result[self.mask] = 0\r\n",
        "        return result\r\n",
        "    \r\n",
        "    def backward(self, dout):\r\n",
        "        dout[self.mask] = 0\r\n",
        "        dx = dout\r\n",
        "        return dx\r\n",
        "\r\n",
        "class Affine:\r\n",
        "    def __init__(self, W, b):\r\n",
        "        self.W = W  #W0, W1\r\n",
        "        self.b = b  #b0, b1\r\n",
        "        self.x = None\r\n",
        "        self.dW = None  #W0, W1의 기울기\r\n",
        "        self.db = None  #b0, b1의 기울기\r\n",
        " \r\n",
        "    def forward(self, x):\r\n",
        "        self.x = x\r\n",
        "        result = np.dot(self.x, self.W) + self.b\r\n",
        "        return result\r\n",
        " \r\n",
        "    def backward(self, dout):\r\n",
        "        dx = np.dot(dout, self.W.T)\r\n",
        "        self.dW = np.dot(self.x.T, dout)\r\n",
        "        self.db = np.sum(dout, axis=0)\r\n",
        "        return dx    \r\n",
        "\r\n",
        "class SoftmaxWithLoss:\r\n",
        "    def __init__(self):\r\n",
        "        self.y = None   # 출력(계산 결과)\r\n",
        "        self.t = None   # 정답(MNIST레이블)\r\n",
        " \r\n",
        "    def foward(self, x, t):\r\n",
        "        self.t = t\r\n",
        "        self.y = softmax(x)\r\n",
        "        result = cee(self.y, self.t)    # 엔트로피 오차 함수를 이용해 오차값\r\n",
        "        return result\r\n",
        " \r\n",
        "    def backward(self, dout=1):     # 역전파 진행\r\n",
        "        batch_size = self.t.shape[0]\r\n",
        "        dx = (self.y - self.t) / batch_size\r\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcMPv47VJjJM"
      },
      "source": [
        "#13.6.5. [STEP5] 클래스 정의 : SimpleNetwork\r\n",
        "\r\n",
        "class SimpleNetwork:\r\n",
        "    def __init__(self, inputx, hidden, outy, weight):\r\n",
        "        #가중치 초기화\r\n",
        "        self.netMat = {}\r\n",
        "        self.netMat['W0'] = weight * np.random.randn(inputx, hidden)\r\n",
        "        self.netMat['b0'] = np.zeros(hidden)\r\n",
        "        self.netMat['W1'] = weight * np.random.randn(hidden, outy)\r\n",
        "        self.netMat['b1'] = np.zeros(outy)\r\n",
        "\r\n",
        "        #계층 생성\r\n",
        "        self.netLayers = {}\r\n",
        "        self.netLayers['Affine1'] = Affine(self.netMat['W0'],\r\n",
        "                                           self.netMat['b0'])\r\n",
        "        self.netLayers['Relu1'] = Relu()\r\n",
        "        self.netLayers['Affine2'] = Affine(self.netMat['W1'],\r\n",
        "                                           self.netMat['b1'])\r\n",
        "        self.netLayers['Softmax'] = SoftmaxWithLoss()\r\n",
        "    \r\n",
        "    def predict(self, x):\r\n",
        "        x = self.netLayers['Affine1'].forward(x)\r\n",
        "        x = self.netLayers['Relu1'].forward(x)\r\n",
        "        x = self.netLayers['Affine2'].forward(x)\r\n",
        "        return x\r\n",
        "    \r\n",
        "    #x: 입력 데이터, t: 정답 레이블\r\n",
        "    def loss(self, x, t):\r\n",
        "        y = self.predict(x)\r\n",
        "        return self.netLayers['Softmax'].forward(y, t)\r\n",
        "    \r\n",
        "    def accuracy(self, x, t):\r\n",
        "        y= self.predict(x)\r\n",
        "        y = np.argmax(y, axis=1)\r\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\r\n",
        "        accuracy = np.sum(y == t) / foat(x.shape[0])\r\n",
        "        return accuracy\r\n",
        "    \r\n",
        "    def numerical_gradient(self, x, t):\r\n",
        "        lossfunc = lambda : self.loss(x,t)\r\n",
        "        grads = {}\r\n",
        "        grads['W0'] = numerical_diff(lossfunc, self.netMat['W0'])\r\n",
        "        grads['b0'] = numerical_diff(lossfunc, self.netMat['b0'])\r\n",
        "        grads['W1'] = numerical_diff(lossfunc, self.netMat['W1'])\r\n",
        "        grads['b1'] = numerical_diff(lossfunc, self.netMat['b1'])\r\n",
        "        return grads\r\n",
        "    \r\n",
        "    def gradient(self, x, t):\r\n",
        "        #forward\r\n",
        "        self.loss(x, t)\r\n",
        "\r\n",
        "        #backward\r\n",
        "        dout = 1\r\n",
        "        dout = self.netLayers['Softmax'].backward(dout)\r\n",
        "        dout = self.netLayers['Affine2'].backward(dout)\r\n",
        "        dout = self.netLayers['Relu1'].backward(dout)\r\n",
        "        dout = self.netLayers['Affine1'].backward(dout)\r\n",
        "\r\n",
        "        #기울기(dW, db) 저장\r\n",
        "        grads = {}\r\n",
        "        grads['W0'] = self.netLayers['Affine1'].dW\r\n",
        "        grads['b0'] = self.netLayers['Affine1'].db\r\n",
        "        grads['W1'] = self.netLayers['Affine2'].dW\r\n",
        "        grads['b1'] = self.netLayers['Affine2'].db\r\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyimqVHvMGDt"
      },
      "source": [
        "#13.6.6 [STEP6] 학습을 위한 설정치 입력\r\n",
        "\r\n",
        "train_size = x_train.shape[0]\r\n",
        "lr = 0.1\r\n",
        "iter = 0\r\n",
        "\r\n",
        "#미분을 사용할 경우 : 배치 20, 1,000회 반복\r\n",
        "#(20개 묶음 데이터로 1,000번 학습 진행)\r\n",
        "iter_num = 0\r\n",
        "if process == 1:\r\n",
        "    iters_num = 1000\r\n",
        "    batch_size = 20\r\n",
        "    iter_per_epoch = 1\r\n",
        "\r\n",
        "#역전파 사용 : 배치 100, 60,000회 반복\r\n",
        "#100개 묶음 데이터로 60,000회 학습 진행\r\n",
        "else:\r\n",
        "    iter_num = 60000\r\n",
        "    batch_size = 100\r\n",
        "    iter_per_epoch = int(train_size/ batch_size)    #600\r\n",
        "\r\n",
        "#MNIST 입력(784), 은닉층(노드 50개), 출력층(노드 10개)\r\n",
        "network = SimpleNetwork(inputx=784, hidden=50, outy=10, weight=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "bgMLNX27M9Ol",
        "outputId": "f44e6308-26bf-44ae-e6f0-ebae85db22a0"
      },
      "source": [
        "#13.6.7. [STEP7] 학습과 검증\r\n",
        "if process == 1:\r\n",
        "    iters_num = 1000\r\n",
        "else:\r\n",
        "    iters_num = 60000\r\n",
        "\r\n",
        "\r\n",
        "#시간 측정 시작\r\n",
        "t1 = time.time()\r\n",
        "print('loss = ______ time = ______ n = ______ | [TrainAcc] [TestAcc]')\r\n",
        "\r\n",
        "\r\n",
        "for i in range(iters_num):\r\n",
        "    batch_mask = np.random.choice(train_size, batch_size)   #60,600개 중 100개\r\n",
        "    x_batch = x_train[batch_mask]\r\n",
        "    t_batch = t_train[batch_mask]\r\n",
        "\r\n",
        "    # 기울기 계산\r\n",
        "\r\n",
        "    if process==1:\r\n",
        "        grad = network.numerical_gradient(x_batch, t_batch) #수치미분 방식\r\n",
        "    else:\r\n",
        "        grad = network.gradient(x_batch, t_batch)   #오차역전파법 방식(훨씬 빠르다)\r\n",
        "    \r\n",
        "    #위에서 만들어진 기울기로 W와 b 갱신\r\n",
        "    for key in ('W0', 'b0', 'W1', 'b1'):\r\n",
        "        network.netMat[key] -= lr*grad[key]\r\n",
        "    \r\n",
        "    loss = network.loss(x_batch, t_batch)\r\n",
        "    #train_loss_list.append(loss)\r\n",
        "\r\n",
        "    if i % iter_per_epoch == 0:\r\n",
        "        train_acc = network.accuracy(x_train, t_train) \r\n",
        "        test_acc = network.accuracy(x_test, t_test)\r\n",
        "        iter = iter + 1\r\n",
        "        print('loss = {7.4f} '.format(loss), end='')\r\n",
        "        print('time = {8.4f} '.format(time.time()-t1), end='')\r\n",
        "        print('n = {:06d} |{:8.4f}{:11.4f}'.format(iter, train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss = ______ time = ______ n = ______ | [TrainAcc] [TestAcc]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-358b62db1494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#수치미분 방식\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#오차역전파법 방식(훨씬 빠르다)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#위에서 만들어진 기울기로 W와 b 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f3a6e538cb9f>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f3a6e538cb9f>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Softmax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SoftmaxWithLoss' object has no attribute 'forward'"
          ]
        }
      ]
    }
  ]
}